# -*- coding: utf-8 -*-
"""Qwen Fine tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hf-u6-oDc0CIXT3KLj6fgCenwtRmnpA-
"""
# source /scratch/chess_ft_env/bin/activate

# Installing dependencies

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen3-8B"
print(model_name)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

# Loads the actual base model from huggingface into colab environment
model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16, device_map="auto")
#model.enable_inputs_require_grads()
import json
from datasets import Dataset

data_path = "analysis.json"

# Load and flatten pairs into simple dicts

with open(data_path) as f:
  data = json.load(f)

max_token_length = 0
for conv in data:
    # convert conversation to full text using tokenizer template
    full_text = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)
    tokenized = tokenizer(full_text, truncation=False)["input_ids"]
    if len(tokenized) > max_token_length:
        max_token_length = len(tokenized)

print("Longest tokenized sequence length:", max_token_length)

wrapped = [{"messages": conv} for conv in data]

ds_train = Dataset.from_list(wrapped)

ignore_index = -100

def format_and_tokenize(example, max_len=max_token_length):
  """example is an index of data"""

  msgs = example["messages"]  # access the two-turn list directly

  # convert conversation int Qwen's chat format
  full_text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)

  # create prefix
  prefix_text = tokenizer.apply_chat_template([msgs[0], {"role": "assistant", "content": ""}], tokenize=False, add_generation_prompt=False)

  # Tokenize with padding
  full = tokenizer(full_text, padding ="max_length", truncation=False, max_length=max_len)
  prefix = tokenizer(prefix_text, padding="max_length", truncation=False, max_length=max_len)

  # Create labels
  input_ids = full["input_ids"]
  labels = input_ids.copy()
  attn = full["attention_mask"]

  #cutoff = len(prefix["input_ids"])
  cutoff = int(sum(prefix["attention_mask"]))
  labels[:cutoff] = [ignore_index] * cutoff

  for i in range(len(labels)):
        if attn[i] == 0:
            labels[i] = ignore_index
    
  return {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": full["attention_mask"],
    }


tok_train = ds_train.map(format_and_tokenize, remove_columns=ds_train.column_names, batched=False)

print("Dataset size:", len(tok_train))

# Pick an example
ex = tok_train[0]

print("\n=== FULL INPUT TEXT ===")
print(tokenizer.decode(ex["input_ids"], skip_special_tokens=False))

print("\n=== LABELS (DECODED, ignoring -100) ===")
label_ids = [t for t in ex["labels"] if t != -100]
print(tokenizer.decode(label_ids, skip_special_tokens=False))

from peft import LoraConfig, get_peft_model

# Configure LoRA (lightweight fine-tuning)
lora_cfg = LoraConfig(
    r=8,                 # rank of LoRA matrices (higher = more capacity)
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj", "k_proj", "v_proj",
        "o_proj", "up_proj", "down_proj", "gate_proj"
    ],
)

# Wrap the base model with LoRA adapters
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

from transformers import TrainingArguments
from trl import SFTTrainer
import torch

bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8
"""
train_args = TrainingArguments(
    output_dir="qwen3-cot-chess-lora",
    num_train_epochs=3,                # increase epochs since dataset is small
    per_device_train_batch_size=1,
    gradient_accumulation_steps=5,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=200,
    gradient_checkpointing=False,
    bf16=bf16_ok,
    fp16=not bf16_ok,
    optim="paged_adamw_8bit" if not bf16_ok else "adamw_torch",
    report_to="none",
)
"""
train_args = TrainingArguments(
    output_dir="qwen3-cot-chess-lora",
    num_train_epochs=5,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    learning_rate=1e-3,
    warmup_ratio=0.1,
    weight_decay=0.01,
    logging_steps=5,
    save_strategy="epoch",
    bf16=bf16_ok,
    fp16=not bf16_ok,
    optim="adamw_torch",
    report_to="none",
)

from transformers import Trainer, default_data_collator

trainer = Trainer(
    model=model,                   # your LoRA-wrapped model
    args=train_args,               # the TrainingArguments you already set
    train_dataset=tok_train,       # tokenized + padded + masked dataset
    data_collator=default_data_collator,  # keeps your labels intact
)
trainer.train()

# Save LoRA adapter (small)
adapter_dir = "qwen3-cot-chess-lora/adapter"
model.save_pretrained(adapter_dir)
tokenizer.save_pretrained(adapter_dir)

# Optionally merge LoRA into base weights (standalone large model)
merged = model.merge_and_unload()
merged_dir = "qwen3-cot-chess-lora/merged"
merged.save_pretrained(merged_dir)
tokenizer.save_pretrained(merged_dir)

print("Saved:", adapter_dir, "and", merged_dir)
"""
def chat_once(fen_text: str):
    messages = [
        {"role":"user","content": f"FEN: {fen_text}\nExplain your reasoning and suggest the best move."}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = merged.generate(**inputs, max_new_tokens=5000, temperature=0.7, top_p=0.9)
    print(tokenizer.decode(out[0], skip_special_tokens=True))

chat_once("b4B2/7R/Pk1pP1p1/6P1/3K1b1p/5P2/4B2r/4n3 w - - 0 1")
"""
import torch

def chat_once(prompt_text: str):
    # Use merged if available, else fall back to model
    m = merged if 'merged' in globals() else model

    # Make sure padding is set to avoid early cutoffs
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    m.config.pad_token_id = tokenizer.pad_token_id

    # Build a single user message with your full prompt
    messages = [{"role": "user", "content": prompt_text}]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(m.device)

    with torch.no_grad():
        out = m.generate(
            **inputs,
            max_new_tokens=2000,   # change if you want longer
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )

    output_text = tokenizer.decode(out[0], skip_special_tokens=True)

    # Show both input and output
    print("==== INPUT ====\n" + prompt_text)
    print("\n==== OUTPUT ====\n" + output_text)

chat_once("<META_GUIDANCE>You are an expert chess player. You will be given board state within the <BOARD> tag. You will be given misc info (en passant, castling, etc.) within the <MISC_INFO> tag. You will be given the turn (white to move or black to move) within the <TURN> tag which indicates which player you are.</META_GUIDANCE><INSTRUCTIONS>You will be RESPONDING with a final move in the format of <MOVE>[your move here as SAN notation string]</MOVE> where the contents of MOVE are SAN notation strings. E.g. to move the knight to c3 you will return <MOVE>Nc3</MOVE>.</INSTRUCTIONS><BOARD>White: Pawns: f3 g5 a6 e6 Bishops: e2 f8 Rook: h7 King: d4 Black: Pawns: h4 d6 g6 Knight: e1 Bishops: f4 a8 Rook: h2 King: b6</BOARD><MISC_INFO><EN_PASSANT><EN_PASSANT>Has legal EP now: False Target square: - Capturing pawn(s): None Moves (SAN notation): None</EN_PASSANT></EN_PASSANT><CAN_CASTLE><CASTLING>Rights: {'white': {'kingside': False, 'queenside': False}, 'black': {'kingside': False, 'queenside': False}} Legal castle moves now (SAN notation): None</CASTLING></CAN_CASTLE></MISC_INFO><TURN>White to move</TURN>")
